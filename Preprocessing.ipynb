{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3356cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c541d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd1db5",
   "metadata": {
    "id": "a5cd1db5"
   },
   "outputs": [],
   "source": [
    "# Read CSV file into a Spark DataFrame\n",
    "df = spark.read.csv(\"file:///home/talentum/shared/Project/hotel_bookings.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef62bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aef62bd",
    "outputId": "5bf117c3-61ed-432a-edde-557e1c0019ca"
   },
   "outputs": [],
   "source": [
    "# Show The DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95405bad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95405bad",
    "outputId": "13c1b806-093c-4e11-d316-373cdaa73a68"
   },
   "outputs": [],
   "source": [
    "# Print the Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of rows\n",
    "num_rows = df.count()\n",
    "\n",
    "# Get number of columns\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(f\"Shape of the DataFrame: ({num_rows}, {num_columns})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699fcff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b699fcff",
    "outputId": "04a37563-ea9a-4658-b1d8-b57af1bc6a14"
   },
   "outputs": [],
   "source": [
    "# Show Basic Statistics for each column\n",
    "for i in df.columns:\n",
    "    df.select(i).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c00e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f7c00e0",
    "outputId": "af9765ff-0dee-4911-b6bd-0196b0913cde"
   },
   "outputs": [],
   "source": [
    "# Print the Unique values and count from each column\n",
    "for i in df.columns:\n",
    "    df.groupBy(i).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f200426",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f200426",
    "outputId": "d01ad4a6-e4c4-4fc0-d237-7231079e74a7"
   },
   "outputs": [],
   "source": [
    "# Printing the schema for a single column\n",
    "df.select(\"arrival_date_month\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6674135",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6674135",
    "outputId": "2c87cd2a-63d5-4c60-cf76-45c29cd1fe9a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(\"arrival_date_month\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dadb4b",
   "metadata": {
    "id": "82dadb4b"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c1585",
   "metadata": {
    "id": "344c1585"
   },
   "outputs": [],
   "source": [
    "# Converting the month column into numeric month\n",
    "df = df.withColumn(\"arrival_date_month\",when(col(\"arrival_date_month\") == \"January\", 1)\n",
    "                   .when(col(\"arrival_date_month\") == \"February\",2)\n",
    "                   .when(col(\"arrival_date_month\") == \"March\",3)\n",
    "                   .when(col(\"arrival_date_month\") == \"April\",4)\n",
    "                   .when(col(\"arrival_date_month\") == \"May\",5)\n",
    "                   .when(col(\"arrival_date_month\") == \"June\",6)\n",
    "                   .when(col(\"arrival_date_month\") == \"July\",7)\n",
    "                   .when(col(\"arrival_date_month\") == \"August\",8)\n",
    "                   .when(col(\"arrival_date_month\") == \"September\",9)\n",
    "                   .when(col(\"arrival_date_month\") == \"October\",10)\n",
    "                   .when(col(\"arrival_date_month\") == \"November\",11)\n",
    "                   .when(col(\"arrival_date_month\") == \"December\",12)\n",
    "                   .otherwise(\"Na\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e72bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "858e72bb",
    "outputId": "2c1bde10-cf0b-4a9f-af15-e6f872dfb991"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"arrival_date_month\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7c86b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0f7c86b",
    "outputId": "4a99f8ce-8707-4c68-aaa0-c755bd8ef0b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(\"arrival_date_month\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d19030",
   "metadata": {
    "id": "86d19030"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2075d3a",
   "metadata": {
    "id": "d2075d3a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Changing Datatype of a Column \"arrival_date_month\"\n",
    "df = df.withColumn(\"arrival_date_month\", df[\"arrival_date_month\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03352a8f",
   "metadata": {
    "id": "03352a8f"
   },
   "outputs": [],
   "source": [
    "# Changing Datatype of a Column \"children\"\n",
    "df = df.withColumn(\"children\", df[\"children\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddd4e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3ddd4e6",
    "outputId": "d03665e7-0a9d-4e09-e2c2-46c955a8e0bb"
   },
   "outputs": [],
   "source": [
    "df.select(\"children\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3180c4-43da-401d-9924-4ffd444ce7b9",
   "metadata": {
    "id": "0a3180c4-43da-401d-9924-4ffd444ce7b9"
   },
   "outputs": [],
   "source": [
    "# Changing Datatype of a Column \"agent\"\n",
    "df = df.withColumn(\"agent\",df[\"agent\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c9e17-7f0d-43fc-9587-5b50410d18c2",
   "metadata": {
    "id": "345c9e17-7f0d-43fc-9587-5b50410d18c2"
   },
   "outputs": [],
   "source": [
    "# Changing Datatype of a Column \"company\"\n",
    "df = df.withColumn(\"company\",df[\"company\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7b6d2-7ab1-4664-aa06-7489b5857783",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98f7b6d2-7ab1-4664-aa06-7489b5857783",
    "outputId": "d9a4f975-f797-4cad-98b4-3727e7dff9fc"
   },
   "outputs": [],
   "source": [
    "# Show the null counts\n",
    "null_counts = {}\n",
    "for c in df.columns:\n",
    "    null_count=df.filter(col(c).isNull()).count()\n",
    "    null_counts[c]=null_count\n",
    "\n",
    "for column, count in null_counts.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aaec36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2aaec36",
    "outputId": "fdd82968-859b-439e-cfd4-4f29587cfba8"
   },
   "outputs": [],
   "source": [
    "# Merging \"Agent\" & \"company\" column based on distribution channel\n",
    "from pyspark.sql.functions import col, when, coalesce\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Distribution_Id\",\n",
    "    when(col(\"distribution_channel\") == \"Direct\", coalesce(col(\"company\"), col(\"agent\")))\n",
    "    .when(col(\"distribution_channel\") == \"Corporate\", coalesce(col(\"company\"), col(\"agent\")))\n",
    "    .when(col(\"distribution_channel\") == \"TA/TO\", coalesce(col(\"agent\"), col(\"company\")))\n",
    "    .otherwise(0) # Default value for unmatched market_segment cases\n",
    ")\n",
    "\n",
    "null_count = df.filter(col('Distribution_Id').isNull()).count()\n",
    "print(f\"Null values in 'Distribution_Id': {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05180b4e-a547-4fff-8196-cfa14d1cad61",
   "metadata": {
    "id": "05180b4e-a547-4fff-8196-cfa14d1cad61"
   },
   "outputs": [],
   "source": [
    "# Filling the null values \n",
    "from pyspark.sql.functions import col, count\n",
    "mode_df = df.groupBy(\"children\").agg(count(\"children\").alias(\"count\")).orderBy(col(\"count\").desc()).limit(1)\n",
    "mode_value = mode_df.collect()[0][\"children\"]\n",
    "df = df.fillna({\"children\": mode_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea2483",
   "metadata": {
    "id": "86ea2483"
   },
   "outputs": [],
   "source": [
    "# Filling the null values\n",
    "from pyspark.sql.functions import col, count\n",
    "mode_df = df.groupBy(\"Distribution_Id\").agg(count(\"Distribution_Id\").alias(\"count\")).orderBy(col(\"count\").desc()).limit(1)\n",
    "mode_value = mode_df.collect()[0][\"Distribution_Id\"]\n",
    "df = df.fillna({\"Distribution_Id\": mode_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692111ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again taking the count of null values\n",
    "null_counts = {}\n",
    "for c in df.columns:\n",
    "    null_count=df.filter(col(c).isNull()).count()\n",
    "    null_counts[c]=null_count\n",
    "\n",
    "for column, count in null_counts.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136651e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "136651e1",
    "outputId": "6a4599c7-4b14-40d1-93fb-c6f142c36ca8"
   },
   "outputs": [],
   "source": [
    "num_rows = df.count()\n",
    "\n",
    "# Get number of columns\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(f\"Shape of DataFrame: ({num_rows}, {num_columns})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XYPpfbvyyP15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYPpfbvyyP15",
    "outputId": "cbf2c55e-15dc-4d8c-eae3-e03a2136732a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating the correlation between output variable and \"reservation status\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"CorrelationExample\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "#data = spark.read.csv(\"Hotel_Booking_Cleaned_Data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert 'reservation_status' column to numeric using StringIndexer\n",
    "indexer = StringIndexer(inputCol='reservation_status', outputCol='reservation_status_index')\n",
    "data = indexer.fit(df).transform(df)\n",
    "\n",
    "# Calculate the correlation between 'is_canceled' and 'reservation_status_index'\n",
    "correlation = data.stat.corr('is_canceled', 'reservation_status_index')\n",
    "print(f\"Correlation between 'is_canceled' and 'reservation_status': {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893754e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the less important columns\n",
    "df = df.drop(\"agent\",\"company\",\"market_segment\",\"reservation_status_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_COpI_V53PQm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_COpI_V53PQm",
    "outputId": "8035b5ae-b2ad-4e9b-c9f4-2d217f3d74d9"
   },
   "outputs": [],
   "source": [
    "# Finding the correlation between all the columns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"HotelBookingCorrelation\").getOrCreate()\n",
    "\n",
    "# Step 2: Load the CSV data into a DataFrame\n",
    "#file_path = \"/content/Hotel_Booking_Cleaned_Data.csv\"\n",
    "#df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Select only numeric columns\n",
    "numeric_columns = [col[0] for col in df.dtypes if col[1] in ('int', 'double')]\n",
    "df_numeric = df.select(numeric_columns)\n",
    "\n",
    "# Step 4: Vectorize the numeric columns\n",
    "assembler = VectorAssembler(inputCols=numeric_columns, outputCol=\"features\")\n",
    "df_vector = assembler.transform(df_numeric).select(\"features\")\n",
    "\n",
    "# Step 5: Calculate the correlation matrix\n",
    "correlation_matrix = Correlation.corr(df_vector, \"features\").head()[0]\n",
    "\n",
    "# Step 6: Convert the correlation matrix to a readable format and show it\n",
    "import numpy as np\n",
    "\n",
    "matrix = np.array(correlation_matrix.toArray())\n",
    "matrix_df = spark.createDataFrame(matrix.tolist(), numeric_columns)\n",
    "matrix_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2H-v4pKw5Jy3",
   "metadata": {
    "id": "2H-v4pKw5Jy3"
   },
   "outputs": [],
   "source": [
    "df.write.csv(\"Cleaned_Data.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
